{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import cv2\n",
    "import glob\n",
    "from skimage import io\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function HOG\n",
    "We start by defining a HOG() function that computes the Histogram of Oriented Gradients of any 64x128 image.\n",
    "The function takes as parameter the path to said image and returns a 3708 elements vector corresponding to the HOG of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOG(path):\n",
    "    img = cv2.imread(path)             # It is necessary here to start by normalizing the image\n",
    "    img = np.float32(img) / 255.0     # All the values are between 0 and 1\n",
    "    kernelX = np.array([[0, 0, 0],\n",
    "                        [-1, 0, 1],    #Here we design two kernels,  \n",
    "                        [0, 0, 0]])    # one to get the \"horizontal\" derivate of the gradient\n",
    "    kernelY = np.array([[0, -1, 0],                        # (derivate with regard to X)\n",
    "                        [0, 0, 0],     # And the other one to get the \"vertical\" derivate of the gradient\n",
    "                        [0, 1, 0]])                                   # (derivate with regard to Y)\n",
    "    tempX = cv2.filter2D(img, -1, kernelX) # We convolute the image with the two kernels \n",
    "    tempY = cv2.filter2D(img, -1, kernelY)    # And set the result in grayscale\n",
    "    gX = cv2.cvtColor(tempX, cv2.COLOR_BGR2GRAY)  # Indeed the color information is not relevant for us\n",
    "    gY = cv2.cvtColor(tempY, cv2.COLOR_BGR2GRAY)   # We get gX and gY the gradient image with regard to X and Y\n",
    "    Norm, Angle = cv2.cartToPolar(gX, gY, angleInDegrees=True)\n",
    "    # This function here allows, using gX and gY, to obtain the Norm and Angle of the gradient.\n",
    "    # It exploits the formulas : Norm = sqrt(gX^2 + gY^2)  and Angle = atan(gY/gX)  (to apply to every pixel)\n",
    "    for i in range(128):\n",
    "        for j in range(64):              # The Angle matrix contains value between 0° and 360°\n",
    "            if Angle[i, j] >= 180:       # Such a wide range of values is useless, therefore we reduce it here to \n",
    "                Angle[i, j] = Angle[i, j] - 180                 # values between 0° (included) and 180° (excluded)\n",
    "    \n",
    "    # We now have the orientation and norm values of the gradient for each pixel\n",
    "    # The next step is to separate the image into 8x8 cells and compute an histogram of oriented gradients for each\n",
    "    cellsHOG = np.array([[[0.0]*9]*8]*16)\n",
    "    # We have 16 rows of 8 cells, and for each one the histogram can be represented as a 9-element vector\n",
    "    # Each element corresponding to a discrete orientation of the gradient.\n",
    "    discAngle = np.array([0, 20, 40, 60, 80, 100, 120, 140, 160])\n",
    "    # This vector represents the 9 discrete values for the orientation of the gradient.\n",
    "    for j in range(0, 128, 8):\n",
    "        for i in range(0, 64, 8):       # Here we are simply creating the 8x8 cells, for both Norm and Angle matrices\n",
    "            cellNorm = Norm[j:j+8, i:i+8]\n",
    "            cellAngle = Angle[j:j+8, i:i+8]\n",
    "            hogCell = np.array([0.0]*9)   # For each cell we represent the histogram as a 9-element vector\n",
    "            for x in range(8):\n",
    "                for y in range(8): # (for each pixel in the cell and for each orientation value)\n",
    "                    for k in range(1, 8):\n",
    "                    # Each pixel's orientation value is between two discrete orientation values.\n",
    "                    # We split the norm value of the gradient between each discrete orientation value,\n",
    "                    # according to the distance between the 2 discrete values and the real one (the one of the pixel).\n",
    "                    # For example, if the real orientation is 10° and the norm is 1, we will put 0.5 in the spot\n",
    "                    # corresponding to 0° and 0.5 in the spot corresponding to 10°.\n",
    "                        if (discAngle[k-1] <= cellAngle[x, y]) and (cellAngle[x, y] <= discAngle[k]) :\n",
    "                            hogCell[k-1] = hogCell[k-1] + (cellAngle[x, y] - discAngle[k-1])*(cellNorm[x, y]/20)\n",
    "                            hogCell[k] = hogCell[k] + (discAngle[k] - cellAngle[x, y])*(cellNorm[x, y]/20)\n",
    "                    # The case where the angle is bigger than 160° is treated almost the same way:\n",
    "                    # It means the angle is between 160° and 180° (180° being the max as we assured before)\n",
    "                    # So we consider 180° as we would consider 0°, and split the norm value between the 160° spot\n",
    "                    # and the 0° spot, according to the distances computed as before, using 180° instead of 0°.\n",
    "                    if cellAngle[x, y] > discAngle[8]:\n",
    "                        hogCell[8] = hogCell[8] + (cellAngle[x, y] - discAngle[8])*(cellNorm[x, y]/20)\n",
    "                        hogCell[0] = hogCell[0] + (180 - cellAngle[x, y])*(cellNorm[x, y]/20)\n",
    "            cellsHOG[j//8, i//8] = hogCell # hogCell represents the hog for the current cell\n",
    "            #cellsHOG is a matrix that contains the HOG of each cell, sorted by position of the cell in the image\n",
    "    \n",
    "    # For the last step we need to design 16x16 overlapping blocks (each containing 2x2 cells)\n",
    "    # (each cell is present in one block at least and four blocks at most)\n",
    "    blocksNormalHOG = np.array([])\n",
    "    for k in range(len(cellsHOG)-1):\n",
    "        for h in range(len(cellsHOG[0])-1):   # Here we design those blocks\n",
    "            vec = np.array([])\n",
    "            for b in range(k, k+2):   # Each block has 4 cells, so it has four 9-element vectors representing a HOG.\n",
    "                for c in range(h, h+2):  # We concatenate these 4 vectors for each block\n",
    "                    vec = np.concatenate((vec, cellsHOG[b, c]))   # And we get a 36-element vector for each block.\n",
    "            vec = vec/np.linalg.norm(vec) # We normalize all these 36-element vectors.\n",
    "            blocksNormalHOG = np.concatenate((blocksNormalHOG, vec))\n",
    "            # And then we just have to concatenate them.\n",
    "            # We had 8x16 cells, which means (since we use overlapping blocks of 2x2 cells) that we have 7x15 blocks.\n",
    "            # The final vector we are returning has then 7x15x36 = 3780 elements.\n",
    "    \n",
    "    return blocksNormalHOG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dataframe using the HOG() function\n",
    "We will use here all the images from the training and test datasets, compute their HOG, labeling them 0 or 1 depending on if they contain humans or not (1 corresponding to the case where they contain a human), and put them in different dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating an empty training dataset\n",
    "train_data = pd.DataFrame(data=None, columns=[\"HOG\", \"label\"])\n",
    "\n",
    "# Then we look at all the images containing humans in the training dataset\n",
    "train_pos_mask = \"Datasets\\\\Train\\\\Pos\\\\*.*\"\n",
    "train_pos_names = glob.glob(train_pos_mask)     # We use the same tool as during the pre-processing\n",
    "for path in train_pos_names:\n",
    "    print(\"processing %s...\" % path,)\n",
    "    data_i = pd.Series([HOG(path), 1], index=train_data.columns)\n",
    "    train_data = train_data.append(data_i, ignore_index=True) # We add the computed data to the dataframe object\n",
    "    # As they are all images containing humans, we label them 1\n",
    "# Now we do the same thing for the images not containing humans in the training dataset\n",
    "train_neg_mask = \"Datasets\\\\Train\\\\Neg\\\\*.*\"\n",
    "train_neg_names = glob.glob(train_neg_mask)\n",
    "for path in train_neg_names:\n",
    "    print(\"processing %s...\" % path,)\n",
    "    data_i = pd.Series([HOG(path), 0], index=train_data.columns)\n",
    "    train_data = train_data.append(data_i, ignore_index=True)\n",
    "    # As they are all images not containing humans, we label them 0\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)  #We shuffle the dataframe and reset the index\n",
    "# The shuffling is necessary because of the way we built this dataset (all the pos then all the neg)\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing Datasets\\Test\\Pos\\crop001501a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001501b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001501c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001501d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001501e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001501f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001501g.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001501h.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001504a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001504b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001504c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001504d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001511a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001511b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001512a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001512b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001512c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001512d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001512e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001512f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001514a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001514b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001514c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001514d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001514e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001514f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001520a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001520b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001520c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001520d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521g.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521h.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521i.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521j.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521k.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001521l.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001522a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001522b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001522c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001522d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001522e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001522f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001531a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001531b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001531c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001531d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001533a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001533b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001533c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001533d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001533e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001533f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001533g.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001533h.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001533i.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001533j.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001544a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001544b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001544c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001544d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001545a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001545b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001546a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001546b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001546c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001546d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001549a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001549b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001555a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001555b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001555c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001555d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001555e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001555f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001555g.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001555h.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001555i.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001555j.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001566a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001566b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001566c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001566d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001573a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001573b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001573c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001573d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001573e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001573f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001573g.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001573h.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001573i.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001573j.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001574a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001574b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001590a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001590b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001593a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001593b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001593c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001593d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001593e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001593f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001593g.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001593h.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001593i.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001593j.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001602a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001602b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001604a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001604b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607g.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607h.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607i.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607j.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607k.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607l.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607m.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607n.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607o.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607p.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607q.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607r.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607s.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001607t.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001631a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001631b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001631c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001631d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001633a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001633b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001633c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001633d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001633e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001633f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001634a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001634b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001638a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001638b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001638c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001638d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001638e.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001638f.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001638g.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001638h.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001639a.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001639b.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001639c.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001639d.jpg...\n",
      "processing Datasets\\Test\\Pos\\crop001639e.jpg...\n"
     ]
    }
   ],
   "source": [
    "# We now do the exact same thing to create the test dataset\n",
    "# We start by creating an empty testing dataset\n",
    "test_data = pd.DataFrame(data=None, columns=[\"HOG\", \"label\"])\n",
    "\n",
    "# Then we look at all the images containing humans in the test dataset\n",
    "test_pos_mask = \"Datasets\\\\Test\\\\Pos\\\\*.*\"\n",
    "test_pos_names = glob.glob(test_pos_mask)     # We use the same tool as during the pre-processing\n",
    "for path in test_pos_names:\n",
    "    print(\"processing %s...\" % path,)\n",
    "    data_i = pd.Series([HOG(path), 1], index=test_data.columns)\n",
    "    test_data = test_data.append(data_i, ignore_index=True)\n",
    "    # As they are all images containing humans, we label them 1\n",
    "# Now we do the same thing for the images not containing humans in the test dataset\n",
    "test_neg_mask = \"Datasets\\\\Test\\\\Neg\\\\*.*\"\n",
    "test_neg_names = glob.glob(test_neg_mask)\n",
    "for path in test_neg_names:\n",
    "    print(\"processing %s...\" % path,)\n",
    "    data_i = pd.Series([HOG(path), 0], index=test_data.columns)\n",
    "    test_data = test_data.append(data_i, ignore_index=True)\n",
    "    # As they are all images not containing humans, we label them 0\n",
    "\n",
    "test_data = test_data.sample(frac=1).reset_index(drop=True)  #We shuffle the dataframe and reset the index\n",
    "# The shuffling is necessary because of the way we built this dataset (all the pos then all the neg)\n",
    "display(test_data) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
