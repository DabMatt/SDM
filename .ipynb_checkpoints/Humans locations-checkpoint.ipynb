{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to test our prototype on a bigger image\n",
    "The idea is to use a sliding window to detect all humans on this bigger picture.\n",
    "We will also use the concept of image pyramid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dill\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the HumanDetector function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Saved\\\\HumanDetector','rb')\n",
    "HumanDetector = dill.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to use pyramid image and sliding windows on this image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding windows only\n",
    "First we design a function to detect humans on an image without changing the scale. It returns the coordinates of the \"windows\" or \"boxes\" where a human has been detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_humans_noScale(img):\n",
    "    windowSize = [128, 64]\n",
    "    stepSizeY = windowSize[0]//16\n",
    "    stepSizeX = windowSize[1]//16\n",
    "    pic = img\n",
    "    rect = np.array([[0, 0, 0, 0]])\n",
    "    for y in range(0, len(pic)-windowSize[0], stepSizeY):\n",
    "        for x in range(0, len(pic[0])-windowSize[1], stepSizeX):\n",
    "            window = pic[y:y+windowSize[0], x:x+windowSize[1], :]\n",
    "            if HumanDetector(window) == 1.0:\n",
    "                rect = np.append(rect, np.array([[x, y, x+windowSize[1], y+windowSize[0]]]), axis=0)\n",
    "    rect = np.delete(rect, 0, 0)\n",
    "    return rect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-scaling\n",
    "Now we will try to detect humans of different sizes by re-scaling the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_scale(pic):\n",
    "    windowSize = [128, 64]\n",
    "    #scale_prop is the scaling factor\n",
    "    scale_prop = 1\n",
    "    # boxes will contain the coordinates of all the boxes where a human has been detected\n",
    "    boxes = np.array([[0, 0, 0, 0]])\n",
    "    while len(pic)>windowSize[0] and len(pic[0])>windowSize[1] and scale_prop>0.2:\n",
    "        width = int(pic.shape[1] * scale_prop)\n",
    "        height = int(pic.shape[0] * scale_prop)\n",
    "        # We compute the new width and height of our image thanks to the scaling factor, and resize the image\n",
    "        dim = (width, height)\n",
    "        pic = cv2.resize(pic, dim, interpolation=cv2.INTER_AREA)\n",
    "        # We compute the list rect of coordinates of bounding boxes, using the previous function\n",
    "        rect = detect_humans_noScale(pic)\n",
    "        # But we have to take the scaling factor into consideration regarding these coordinates\n",
    "        # rect_real will be the list of coordinates of those bounding boxes in the original image\n",
    "        rect_real = rect\n",
    "        for i in range(len(rect)):\n",
    "            for j in range(4):\n",
    "                rect_real[i, j] = rect[i, j]*(1/scale_prop)\n",
    "        boxes = np.append(boxes, rect_real, axis=0)\n",
    "        scale_prop = scale_prop - 0.1\n",
    "    boxes = np.delete(boxes, 0, 0)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling overlapping boxes\n",
    "We use a non-maxima suppression algorithm to handle the fact that we have several \"boxes\" or \"windows\" corresponding to the same human.\n",
    "This function has to be used with the position of the boxes where humans were detected as input, along with an overlapping threshold (usually 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "    # We convert all the integers in the list of boxes into float\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "    # This list will contain indexes of picked boxes\n",
    "    pick = []\n",
    "    # We get the coordinates from the list of boxes that we got from human detection\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    # We compute the area of the boxes\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    # We sort the list of boxes by the y2 coordinate.\n",
    "    # More precisely, idxs is the list of indexes of the boxes in the original list, \n",
    "    # sorted according to the y2 coordinate, in non-decreasing order.\n",
    "    idxs = np.argsort(y2)\n",
    "    # The idxs list will end up empty.\n",
    "    \n",
    "    while len(idxs) > 0:\n",
    "        # last is the last index in the 'indxs' list\n",
    "        # i is the index from the list 'boxes' corresponding to the last index in 'indx' \n",
    "        # We select i, and add it to the list 'pick'.\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        # i allows us to access to coordinates of a certain box, that we will call the picked box.\n",
    "        # The picked box changes at each iteration of the while loop.\n",
    "        pick.append(i)\n",
    "        \n",
    "        # We find the largest (x1, y1) coordinates for the start of the box and \n",
    "        # the smallest (x2, y2) coordinates for the end of the box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        # This commands allows us to obtain a list of maximas: x[i] is compared to every other x[j]\n",
    "        # And for every comparison, the max is put in the list 'xx1'.\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "        # For each comparison between the picked box and an other box, the coordinates 'xx1' to 'yy2' correspond to \n",
    "        # the coordinates of the biggest box contained in both the picked box and the other box, if it exists.\n",
    "        # Its area is the overlapping area of the two considered boxes.\n",
    "        \n",
    "        # w and h are respectively the width and height of the boxes obtained thanks to the max and min.\n",
    "        # More precisely, they are lists containing those width and height, as we have lists containing the\n",
    "        # coordinates of the boxes.\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        # overlap is a list containing the overlapping ratios of every box with the picked box.\n",
    "        # If two considered blocks are not overlapping, their overlapping ratio should be negative.\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "        # From the list idxs, we delete the index of the picked box as well as the indexes of boxes overlapping\n",
    "        # more than overlapThresh with the picked box.\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "        \n",
    "    # pick contains the indexes of all the boxes that have been picked in the process.\n",
    "    # Therefore those boxes are not overlapping with each other more than overlapThresh,\n",
    "    # Since all such boxes have been deleted already.\n",
    "    return boxes[pick].astype(\"int\")\n",
    "\n",
    "# We need to say here that if we had chosen an other way to choose the boxes initially picked,\n",
    "# the result would have been different. Indeed, there are several solutions to this problem:\n",
    "# if we consider two boxes b1 and b2 overlapping more than overlapThresh, keeping any one of b1 and b2\n",
    "# is a solution to our problem, as long as we pick only one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the results and computing the coordinates of the humans\n",
    "Computing these coordinates can be useful to compute distances between people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_detection(image):\n",
    "    img = image\n",
    "    windowSize = [128, 64]\n",
    "    # Those coordinates are those of overlapping bounding boxes\n",
    "    rect_overlap = detect_with_scale(img)\n",
    "    # We apply the non-maxima suppression algo\n",
    "    rect = non_max_suppression_fast(rect_overlap, 0.5)\n",
    "    n = len(rect)\n",
    "    coord = np.array([[0, 0]]*n)\n",
    "    for i in range(n):\n",
    "        x1 = rect[i, 0]\n",
    "        y1 = rect[i, 1]\n",
    "        x2 = rect[i, 2]\n",
    "        y2 = rect[i, 3]\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2) # We draw rectangles on img\n",
    "        coord[i] = [(x1 + x2)//2, y2]\n",
    "        # The coordinates of the human are calculated this way so we can agree to consider their coordinates on the\n",
    "        # ground, which should be useful for later.\n",
    "    # We return rect as well because it could be useful later, as it allows us to have access to the coordinates of\n",
    "    # the bounding boxes containing humans, ordered the same way as the list containing the coordinates of the humans\n",
    "    return [img, coord, rect]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the image we will use and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(\"Pedestrians\\\\Registered_01.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[result, coord, rect] = human_detection(img1)\n",
    "# The execution of this cell couldn't be recorded as it takes too much time, but it was indeed executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coord : \", coord)\n",
    "print(\"rect : \", rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis(\"off\")\n",
    "plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Saved\\\\Human_locations_session','wb')\n",
    "dill.dump_session(file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
