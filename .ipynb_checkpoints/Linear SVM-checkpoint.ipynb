{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design of the linear SVM model\n",
    "Here we design by ourselves a linear SVM model adapted to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for training and testing\n",
    "### Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"Datasets\\\\Train\\\\train_data.csv\", index_col=0)\n",
    "test_data = pd.read_csv(\"Datasets\\\\Test\\\\test_data.csv\", index_col=0)\n",
    "train_data = train_data.dropna() # We drop all invalid values,\n",
    "test_data = test_data.dropna()   # whatever phase of the dataframes building they come from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating X and Y for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"label\", axis=\"columns\")\n",
    "Y_train = train_data[\"label\"]\n",
    "X_test = test_data.drop(\"label\", axis=\"columns\")\n",
    "Y_test = test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the SVM model through object-oriented programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_SVM:\n",
    "    # The init function will be useful to create the SVM object and will do the fitting as well\n",
    "    def __init__(self, X, y, iter_nb, lr, C):\n",
    "        \n",
    "        self.X = X.values\n",
    "        self.y = y.values\n",
    "        self.iter_nb = iter_nb\n",
    "        # This is simply the number of iterations to do\n",
    "        self.lr = lr\n",
    "        # lr is the learning rate for the gradient descent approach used here\n",
    "        self.C = C\n",
    "        # C is a coeff defining the impact of each support vector on the updating of the vector w\n",
    "        \n",
    "        # Initialize normal vector by setting all its element at 1, then w will be reduced during the fitting phase\n",
    "        # The vector w defines the hyperplane separating the 'pos' samples from the 'neg' ones\n",
    "        # (w is actually the vector normal to the hyperplane)\n",
    "        # Its length has to be the same as any vector sample from the training set as it belongs to the same space\n",
    "        self.w = np.ones(len(self.X[0]))    \n",
    "    \n",
    "    def distances(self, w):\n",
    "        # We compute the distances between the sample and the current hyperplane defined by the current vector w\n",
    "        # We know that (xi.w +b) >= 1 or <= -1 according to the sample being 'pos' or 'neg'\n",
    "        # Since, for all 'neg' samples, yi = -1, and for all 'pos' samples yi = 1\n",
    "        # The absolute distance of a sample (xi, yi) to the hyperplane's margin corresponds to yi*(xi.w + b) - 1\n",
    "        # (This is true because we considered the width of the margin to be 1)\n",
    "        distances = self.y * (np.dot(self.X, w)) - 1\n",
    "        \n",
    "        # The support vectors (located between the two margins) have a distance inferior to 0\n",
    "        # (Such distances exist in our dataset only if w is not optimal yet)\n",
    "        distances[distances > 0] = 0\n",
    "        # All vectors that aren't on support vectors now have distances equal to 0\n",
    "        # This will allow us to identify which vectors are on the support vector\n",
    "        # And will allow us to apply the Lagrange multiplier\n",
    "\n",
    "        return distances\n",
    "    \n",
    "    def get_cost_grads(self, X, w, y):\n",
    "        \n",
    "        # We compute a list of distances defined by the previous function\n",
    "        distances = self.distances(w)\n",
    "\n",
    "        # L is the current cost defined by the following formula:\n",
    "        # L = |w|Â²/2 - sum(distance_i*coeff_i)\n",
    "        L = 1 / 2 * np.dot(w, w) - self.C * np.sum(distances)\n",
    "        \n",
    "        dw = np.zeros(len(w))\n",
    "        \n",
    "        # We use the index and value of each element in the list \"distances\"\n",
    "        for index, val in enumerate(distances):\n",
    "            if val == 0:  \n",
    "                # As said earlier, this means the sample is not on the support vector\n",
    "                # (alpha = 0 for Lagrange multiplier)\n",
    "                di = w  # We keep the initial value of w as di, no need to change it here\n",
    "                # In other words, the current value of w \"works\" for this sample\n",
    "                # (we are applying the gradient descent method with the Lagrange multiplier)\n",
    "            else:\n",
    "                # Here we have a support vector, it's not supposed to happen with an optimal value of w\n",
    "                # (alpha = 1 for Lagrange multiplier)\n",
    "                di = w - (self.C * y[index] * X[index]) # We compute an other value of a vector normal to\n",
    "                # the optimal hyperplane, according to the gradient descent method\n",
    "            dw += di\n",
    "        return L, dw / len(X) #dw corresponds to a mean of all the di, and will be used to update the value of w\n",
    "        # L is the objective cost function, and dw/len(X) is the gradient\n",
    "    \n",
    "    def fit(self):   \n",
    "        # This is the fitting phase, where we find the optimal \n",
    "        # We obviously do a number of iterations equal to iter_nb\n",
    "        # In reality we rarely can get the optimal value of w but we can get very close\n",
    "        for i in range(self.iter_nb):\n",
    "            L, dw = self.get_cost_grads(self.X, self.w, self.y)\n",
    "            # We compute the cost function and the gradient of w at each iteration\n",
    "            self.w = self.w - self.lr * dw\n",
    "            # We update the value of w using the learning rate and the gradient\n",
    "            # The gradient and cost values change at each iteration because w changes\n",
    "            # (And because the \"distances\" list changes as well as a result of w being updated)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(X @ self.w)\n",
    "        # The value of y_i corresponds to the sign of x_i.w, as it has been explained before\n",
    "    \n",
    "    def test(self, X_t, Y_t):\n",
    "        X = X_t.values\n",
    "        Y = Y_t.values\n",
    "        comp = self.predict(X)-Y.flatten()\n",
    "        # We make sure Y is a one-dimension array, or else the comparison won't work\n",
    "        accuracy = len(np.where(comp == 0)[0]) / len(comp)\n",
    "        # The accuracy depends on the comparison between the prediction made from X and the real value of Y\n",
    "        print(\"Test Accuracy :\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = Linear_SVM(X=X_train, y=Y_train, iter_nb=1000, lr=1e-3, C=30)\n",
    "SVM.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Saved\\\\SVM','wb')\n",
    "dill.dump(SVM, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM.test(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and saving a Human Detector function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = open('Saved\\\\SVM','rb')\n",
    "SVM = dill.load(file2)\n",
    "file2.close()\n",
    "\n",
    "file3 = open('Saved\\\\HOG','rb')\n",
    "HOG = dill.load(file3)\n",
    "file3.close()\n",
    "\n",
    "def HumanDetector(img):\n",
    "    return SVM.predict(HOG(img))\n",
    "\n",
    "file4 = open('Saved\\\\HumanDetector','wb')\n",
    "dill.dump(HumanDetector, file4)\n",
    "file4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"crop_000010a.jpg\")\n",
    "HumanDetector(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.imread(\"00000006a_patch_0_.png\")\n",
    "HumanDetector(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
