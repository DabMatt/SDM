{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design of the linear SVM model\n",
    "Here we design by ourselves a linear SVM model adapted to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for training and testing\n",
    "### Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOG</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[0.07733569 0.1750842  0.05174413 ... 0.101580...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[0.17419553 0.1873917  0.09923255 ... 0.078406...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[0.11382956 0.0785783  0.03860593 ... 0.045908...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[0.1484477  0.32133852 0.14553327 ... 0.024032...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[0.10410056 0.17956561 0.05774328 ... 0.090186...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14591</td>\n",
       "      <td>[0.33211777 0.64055494 0.04186667 ... 0.087455...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14592</td>\n",
       "      <td>[0.         0.04934804 0.03757627 ... 0.016498...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14593</td>\n",
       "      <td>[0.         0.48937748 0.13842299 ... 0.165235...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14594</td>\n",
       "      <td>[0.         0.34095434 0.         ... 0.115503...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14595</td>\n",
       "      <td>[0.03000964 0.10683461 0.04054548 ... 0.092261...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14596 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     HOG  label\n",
       "0      [0.07733569 0.1750842  0.05174413 ... 0.101580...      1\n",
       "1      [0.17419553 0.1873917  0.09923255 ... 0.078406...      0\n",
       "2      [0.11382956 0.0785783  0.03860593 ... 0.045908...      0\n",
       "3      [0.1484477  0.32133852 0.14553327 ... 0.024032...      0\n",
       "4      [0.10410056 0.17956561 0.05774328 ... 0.090186...      0\n",
       "...                                                  ...    ...\n",
       "14591  [0.33211777 0.64055494 0.04186667 ... 0.087455...      0\n",
       "14592  [0.         0.04934804 0.03757627 ... 0.016498...      0\n",
       "14593  [0.         0.48937748 0.13842299 ... 0.165235...      0\n",
       "14594  [0.         0.34095434 0.         ... 0.115503...      0\n",
       "14595  [0.03000964 0.10683461 0.04054548 ... 0.092261...      0\n",
       "\n",
       "[14596 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"Datasets\\\\Train\\\\train_data.csv\", index_col=0)\n",
    "test_data = pd.read_csv(\"Datasets\\\\Test\\\\test_data.csv\", index_col=0)\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating X and Y for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(\"label\", axis=1)\n",
    "Y_train = train_data[\"label\"]\n",
    "X_test = test_data.drop(\"label\", axis=1)\n",
    "Y_test = test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the SVM model through object-oriented programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    # The init function will be useful to create the SVM object and will do the fitting as well\n",
    "    def __init__(self, X, y, iter_nb, lr, C):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.iter_nb = iter_nb\n",
    "        # This is simply the number of iterations to do\n",
    "        self.lr = lr\n",
    "        # lr is the learning rate for the gradient descent approach used here\n",
    "        self.C = C\n",
    "        # C is a coeff defining the impact of each support vector on the updating of the vector w\n",
    "        \n",
    "        # Initialize normal vector by setting all its element at 1, then w will be reduced during the fitting phase\n",
    "        # The vector w defines the hyperplane separating the 'pos' samples from the 'neg' ones\n",
    "        # (w is actually the vector normal to the hyperplane)\n",
    "        # Its length has to be the same as any vector sample from the training set as it belongs to the same space\n",
    "        self.w = np.ones(len(self.X[0]))\n",
    "        self.fit()\n",
    "    \n",
    "    \n",
    "    def distances(self, w):\n",
    "        # We compute the distances between the sample and the current hyperplane defined by the current vector w\n",
    "        # We know that (xi.w +b) >= 1 or <= -1 according to the sample being 'pos' or 'neg'\n",
    "        # Since, for all 'neg' samples, yi = -1, and for all 'pos' samples yi = 1\n",
    "        # The absolute distance of a sample (xi, yi) to the hyperplane's margin corresponds to yi*(xi.w + b) - 1\n",
    "        # (This is true because we considered the width of the margin to be 1)\n",
    "        distances = self.y * (np.dot(self.X, w)) - 1\n",
    "        \n",
    "        # The support vectors (located between the two margins) have a distance inferior to 0\n",
    "        # (Such distances exist in our dataset only if w is not optimal yet)\n",
    "        distances[distances > 0] = 0\n",
    "        # All vectors that aren't on support vectors now have distances equal to 0\n",
    "        # This will allow us to identify which vectors are on the support vector\n",
    "        # And will allow us to apply the Lagrange multiplier\n",
    "\n",
    "        return distances\n",
    "    \n",
    "    def get_cost_grads(self, X, w, y):\n",
    "        \n",
    "        # We compute a list of distances defined by the previous function\n",
    "        distances = self.distances(w)\n",
    "\n",
    "        # L is the current cost defined by the following formula:\n",
    "        # L = |w|²/2 - sum(distance_i*coeff_i)\n",
    "        L = 1 / 2 * np.dot(w, w) - self.C * np.sum(distances)\n",
    "        \n",
    "        dw = np.zeros(len(w))\n",
    "        \n",
    "        # We use the index and value of each element in the list \"distances\"\n",
    "        for index, val in enumerate(distances):\n",
    "            if val == 0:  \n",
    "                # As said earlier, this means the sample is not on the support vector\n",
    "                # (alpha = 0 for Lagrange multiplier)\n",
    "                di = w  # We keep the initial value of w as di, no need to change it here\n",
    "                # In other words, the current value of w \"works\" for this sample\n",
    "                # (we are applying the gradient descent method with the Lagrange multiplier)\n",
    "            else:\n",
    "                # Here we have a support vector, it's not supposed to happen with an optimal value of w\n",
    "                # (alpha = 1 for Lagrange multiplier)\n",
    "                di = w - (self.C * y[index] * X[index]) # We compute an other value of a vector normal to\n",
    "                # the optimal hyperplane, according to the gradient descent method\n",
    "            dw += di\n",
    "        return L, dw / len(X) #dw corresponds to a mean of all the di, and will be used to update the value of w\n",
    "        # L is the objective cost function, and dw/len(X) is the gradient\n",
    "    \n",
    "    def fit():   \n",
    "        # This is the fitting phase, where we find the optimal \n",
    "        # We obviously do a number of iterations equal to iter_nb\n",
    "        # In reality we rarely can get the optimal value of w but we can get very close\n",
    "        for i in range(self.iter_nb):\n",
    "            L, dw = self.get_cost_grads(self.X, self.w, self.y)\n",
    "            # We compute the cost function and the gradient of w at each iteration\n",
    "            self.w = self.w - self.lr * dw\n",
    "            # We update the value of w using the learning rate and the gradient\n",
    "            # The gradient and cost values change at each iteration because w changes\n",
    "            # (And because the \"distances\" list changes as well as a result of w being updated)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(X @ self.w)\n",
    "        # The value of y_i corresponds to the sign of x_i.w, as it has been explained before\n",
    "    \n",
    "    def test(self, X, Y):\n",
    "        comp = self.predict(X)-Y.flatten()\n",
    "        # We make sure Y is a one-dimension array, or else the comparison won't work\n",
    "        accuracy = len(np.where(comp == 0)[0]) / len(comp)\n",
    "        # The accuracy depends on the comparison between the prediction made from X and the real value of Y\n",
    "        print(\"Test Accuracy :\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
