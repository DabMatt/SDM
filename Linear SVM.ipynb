{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design of the linear SVM model\n",
    "Here we design by ourselves a linear SVM model adapted to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for training and testing\n",
    "### Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"Datasets\\\\Train\\\\train_data.csv\", index_col=0)\n",
    "test_data = pd.read_csv(\"Datasets\\\\Test\\\\test_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating X and Y for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[\"HOG\"]\n",
    "Y_train = train_data[\"label\"]\n",
    "X_test = test_data[\"HOG\"]\n",
    "Y_test = test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the SVM model through object-oriented programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_SVM:\n",
    "    # The init function will be useful to create the SVM object and will do the fitting as well\n",
    "    def __init__(self, X, y, iter_nb, lr, C):\n",
    "        \n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        self.iter_nb = iter_nb\n",
    "        # This is simply the number of iterations to do\n",
    "        self.lr = lr\n",
    "        # lr is the learning rate for the gradient descent approach used here\n",
    "        self.C = C\n",
    "        # C is a coeff defining the impact of each support vector on the updating of the vector w\n",
    "        \n",
    "        # Initialize normal vector by setting all its element at 1, then w will be reduced during the fitting phase\n",
    "        # The vector w defines the hyperplane separating the 'pos' samples from the 'neg' ones\n",
    "        # (w is actually the vector normal to the hyperplane)\n",
    "        # Its length has to be the same as any vector sample from the training set as it belongs to the same space\n",
    "        self.w = np.ones(len(self.X[0]))\n",
    "    \n",
    "    \n",
    "    def distances(self, w):\n",
    "        # We compute the distances between the sample and the current hyperplane defined by the current vector w\n",
    "        # We know that (xi.w +b) >= 1 or <= -1 according to the sample being 'pos' or 'neg'\n",
    "        # Since, for all 'neg' samples, yi = -1, and for all 'pos' samples yi = 1\n",
    "        # The absolute distance of a sample (xi, yi) to the hyperplane's margin corresponds to yi*(xi.w + b) - 1\n",
    "        # (This is true because we considered the width of the margin to be 1)\n",
    "        distances = self.y * (np.dot(self.X, w)) - 1\n",
    "        \n",
    "        # The support vectors (located between the two margins) have a distance inferior to 0\n",
    "        # (Such distances exist in our dataset only if w is not optimal yet)\n",
    "        distances[distances > 0] = 0\n",
    "        # All vectors that aren't on support vectors now have distances equal to 0\n",
    "        # This will allow us to identify which vectors are on the support vector\n",
    "        # And will allow us to apply the Lagrange multiplier\n",
    "\n",
    "        return distances\n",
    "    \n",
    "    def get_cost_grads(self, X, w, y):\n",
    "        \n",
    "        # We compute a list of distances defined by the previous function\n",
    "        distances = self.distances(w)\n",
    "\n",
    "        # L is the current cost defined by the following formula:\n",
    "        # L = |w|Â²/2 - sum(distance_i*coeff_i)\n",
    "        L = 1 / 2 * np.dot(w, w) - self.C * np.sum(distances)\n",
    "        \n",
    "        dw = np.zeros(len(w))\n",
    "        \n",
    "        # We use the index and value of each element in the list \"distances\"\n",
    "        for index, val in enumerate(distances):\n",
    "            if val == 0:  \n",
    "                # As said earlier, this means the sample is not on the support vector\n",
    "                # (alpha = 0 for Lagrange multiplier)\n",
    "                di = w  # We keep the initial value of w as di, no need to change it here\n",
    "                # In other words, the current value of w \"works\" for this sample\n",
    "                # (we are applying the gradient descent method with the Lagrange multiplier)\n",
    "            else:\n",
    "                # Here we have a support vector, it's not supposed to happen with an optimal value of w\n",
    "                # (alpha = 1 for Lagrange multiplier)\n",
    "                di = w - (self.C * y[index] * X[index]) # We compute an other value of a vector normal to\n",
    "                # the optimal hyperplane, according to the gradient descent method\n",
    "            dw += di\n",
    "        return L, dw / len(X) #dw corresponds to a mean of all the di, and will be used to update the value of w\n",
    "        # L is the objective cost function, and dw/len(X) is the gradient\n",
    "    \n",
    "    def fit(self):   \n",
    "        # This is the fitting phase, where we find the optimal \n",
    "        # We obviously do a number of iterations equal to iter_nb\n",
    "        # In reality we rarely can get the optimal value of w but we can get very close\n",
    "        for i in range(self.iter_nb):\n",
    "            L, dw = self.get_cost_grads(self.X, self.w, self.y)\n",
    "            # We compute the cost function and the gradient of w at each iteration\n",
    "            self.w = self.w - self.lr * dw\n",
    "            # We update the value of w using the learning rate and the gradient\n",
    "            # The gradient and cost values change at each iteration because w changes\n",
    "            # (And because the \"distances\" list changes as well as a result of w being updated)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(X @ self.w)\n",
    "        # The value of y_i corresponds to the sign of x_i.w, as it has been explained before\n",
    "    \n",
    "    def test(self, X, Y):\n",
    "        comp = self.predict(X)-Y.flatten()\n",
    "        # We make sure Y is a one-dimension array, or else the comparison won't work\n",
    "        accuracy = len(np.where(comp == 0)[0]) / len(comp)\n",
    "        # The accuracy depends on the comparison between the prediction made from X and the real value of Y\n",
    "        print(\"Test Accuracy :\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (14596,) and (71,) not aligned: 14596 (dim 0) != 71 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-bd4cbeef8553>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mSVM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinear_SVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter_nb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mSVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-103-956266243552>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# In reality we rarely can get the optimal value of w but we can get very close\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_nb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cost_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;31m# We compute the cost function and the gradient of w at each iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-103-956266243552>\u001b[0m in \u001b[0;36mget_cost_grads\u001b[1;34m(self, X, w, y)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# We compute a list of distances defined by the previous function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# L is the current cost defined by the following formula:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-103-956266243552>\u001b[0m in \u001b[0;36mdistances\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# The absolute distance of a sample (xi, yi) to the hyperplane's margin corresponds to yi*(xi.w + b) - 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# (This is true because we considered the width of the margin to be 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# The support vectors (located between the two margins) have a distance inferior to 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (14596,) and (71,) not aligned: 14596 (dim 0) != 71 (dim 0)"
     ]
    }
   ],
   "source": [
    "SVM = Linear_SVM(X=X_train, y=Y_train, iter_nb=10000, lr=1e-3, C=30)\n",
    "SVM.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.         0.         0.         ... 0.13701964 0.04555695 0.        ]'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
